"""
IEEE Xplore çˆ¬å–ç»“æœåˆ†æè„šæœ¬
ç”¨äºåˆå¹¶ã€ç»Ÿè®¡å’Œå¯¼å‡ºçˆ¬å–çš„æ–‡çŒ®æ•°æ®
"""

import json
import glob
import os
from datetime import datetime
import csv

class ResultAnalyzer:
    def __init__(self, results_dir='ieee_results'):
        self.results_dir = results_dir
        self.all_articles = []
        self.query_stats = []
        
    def load_all_results(self):
        """åŠ è½½æ‰€æœ‰ç»“æœæ–‡ä»¶"""
        result_files = glob.glob(f"{self.results_dir}/query_*_results.json")
        result_files.sort(key=lambda x: int(x.split('_')[1]))  # æŒ‰ç¼–å·æ’åº
        
        print(f"æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        for file_path in result_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    query_stat = {
                        'query_id': data['query_id'],
                        'query_text': data['query_text'][:100] + '...' if len(data['query_text']) > 100 else data['query_text'],
                        'total_results': data.get('total_results', 'N/A'),
                        'articles_count': data.get('articles_count', 0),
                        'crawl_time': data.get('crawl_time', 'N/A')
                    }
                    self.query_stats.append(query_stat)
                    
                    # æ”¶é›†æ‰€æœ‰æ–‡ç« ï¼ˆæ·»åŠ æ¥æºæ£€ç´¢å¼ä¿¡æ¯ï¼‰
                    for article in data.get('articles', []):
                        article['source_query_id'] = data['query_id']
                        article['source_query_text'] = data['query_text']
                        self.all_articles.append(article)
                        
                print(f"âœ“ å·²åŠ è½½ï¼š{file_path} - {data.get('articles_count', 0)} ç¯‡æ–‡ç« ")
                
            except Exception as e:
                print(f"âœ— åŠ è½½å¤±è´¥ï¼š{file_path} - {e}")
        
        print(f"\næ€»å…±åŠ è½½äº† {len(self.all_articles)} ç¯‡æ–‡çŒ®")
        
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡")
        print("="*80)
        
        print(f"\næ£€ç´¢å¼æ•°é‡ï¼š{len(self.query_stats)}")
        print(f"æ–‡çŒ®æ€»æ•°ï¼š{len(self.all_articles)}")
        
        # æŒ‰æ£€ç´¢å¼ç»Ÿè®¡
        print(f"\n{'æ£€ç´¢å¼ID':<8} {'æ–‡çŒ®æ•°':<8} {'æ€»ç»“æœæ•°':<15} {'çˆ¬å–æ—¶é—´'}")
        print("-"*80)
        
        total_articles = 0
        for stat in self.query_stats:
            print(f"{stat['query_id']:<8} {stat['articles_count']:<8} {str(stat['total_results']):<15} {stat['crawl_time'][:19]}")
            total_articles += stat['articles_count']
        
        print("-"*80)
        print(f"{'åˆè®¡':<8} {total_articles:<8}")
        
        # å¹´ä»½åˆ†å¸ƒ
        years = [a.get('year', 'N/A') for a in self.all_articles]
        year_counts = {}
        for year in years:
            if year != 'N/A':
                year_counts[year] = year_counts.get(year, 0) + 1
        
        if year_counts:
            print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒï¼ˆTop 10ï¼‰ï¼š")
            sorted_years = sorted(year_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            for year, count in sorted_years:
                print(f"  {year}: {count} ç¯‡")
        
        # å»é‡ç»Ÿè®¡
        unique_titles = set(a.get('title', '') for a in self.all_articles)
        print(f"\nğŸ”„ å»é‡åæ–‡çŒ®æ•°ï¼š{len(unique_titles)} ç¯‡")
        print(f"   é‡å¤æ–‡çŒ®æ•°ï¼š{len(self.all_articles) - len(unique_titles)} ç¯‡")
        
    def remove_duplicates(self):
        """å»é™¤é‡å¤æ–‡çŒ®ï¼ˆåŸºäºæ ‡é¢˜ï¼‰"""
        seen_titles = set()
        unique_articles = []
        
        for article in self.all_articles:
            title = article.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        
        print(f"\nå»é‡å‰ï¼š{len(self.all_articles)} ç¯‡")
        print(f"å»é‡åï¼š{len(unique_articles)} ç¯‡")
        
        return unique_articles
    
    def export_to_csv(self, output_file='all_articles.csv', remove_duplicates=True):
        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        if not self.all_articles:
            print("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        articles = self.remove_duplicates() if remove_duplicates else self.all_articles
        
        print(f"\nğŸ“ æ­£åœ¨å¯¼å‡ºåˆ° {output_file}...")
        
        # å®šä¹‰CSVå­—æ®µ
        fieldnames = [
            'title', 'authors', 'year', 'publisher_info', 
            'abstract', 'link', 'source_query_id', 'source_query_text'
        ]
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for article in articles:
                row = {field: article.get(field, 'N/A') for field in fieldnames}
                writer.writerow(row)
        
        print(f"âœ“ å¯¼å‡ºå®Œæˆï¼š{len(articles)} ç¯‡æ–‡çŒ®")
        print(f"  æ–‡ä»¶ä½ç½®ï¼š{os.path.abspath(output_file)}")
    
    def export_to_excel(self, output_file='all_articles.xlsx', remove_duplicates=True):
        """å¯¼å‡ºä¸ºExcelæ–‡ä»¶ï¼ˆéœ€è¦pandaså’Œopenpyxlï¼‰"""
        try:
            import pandas as pd
            
            if not self.all_articles:
                print("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
                return
            
            articles = self.remove_duplicates() if remove_duplicates else self.all_articles
            
            print(f"\nğŸ“Š æ­£åœ¨å¯¼å‡ºåˆ° {output_file}...")
            
            df = pd.DataFrame(articles)
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            column_order = [
                'title', 'authors', 'year', 'publisher_info', 
                'abstract', 'link', 'source_query_id', 'source_query_text'
            ]
            
            # åªä¿ç•™å­˜åœ¨çš„åˆ—
            column_order = [col for col in column_order if col in df.columns]
            df = df[column_order]
            
            # å¯¼å‡º
            df.to_excel(output_file, index=False, engine='openpyxl')
            
            print(f"âœ“ å¯¼å‡ºå®Œæˆï¼š{len(articles)} ç¯‡æ–‡çŒ®")
            print(f"  æ–‡ä»¶ä½ç½®ï¼š{os.path.abspath(output_file)}")
            
        except ImportError:
            print("âœ— éœ€è¦å®‰è£… pandas å’Œ openpyxlï¼š")
            print("  pip install pandas openpyxl")
    
    def export_query_stats(self, output_file='query_statistics.csv'):
        """å¯¼å‡ºæ£€ç´¢å¼ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“‹ æ­£åœ¨å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ° {output_file}...")
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            fieldnames = ['query_id', 'query_text', 'total_results', 'articles_count', 'crawl_time']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.query_stats)
        
        print(f"âœ“ å¯¼å‡ºå®Œæˆ")
        print(f"  æ–‡ä»¶ä½ç½®ï¼š{os.path.abspath(output_file)}")
    
    def search_by_keyword(self, keyword):
        """æŒ‰å…³é”®è¯æœç´¢æ–‡çŒ®"""
        results = []
        keyword_lower = keyword.lower()
        
        for article in self.all_articles:
            title = article.get('title', '').lower()
            abstract = article.get('abstract', '').lower()
            
            if keyword_lower in title or keyword_lower in abstract:
                results.append(article)
        
        print(f"\nğŸ” æœç´¢å…³é”®è¯ '{keyword}'ï¼šæ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³æ–‡çŒ®")
        
        for idx, article in enumerate(results[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"\n{idx}. {article.get('title', 'N/A')}")
            print(f"   ä½œè€…ï¼š{article.get('authors', 'N/A')}")
            print(f"   å¹´ä»½ï¼š{article.get('year', 'N/A')}")
        
        if len(results) > 10:
            print(f"\n... è¿˜æœ‰ {len(results) - 10} ç¯‡")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         IEEE Xplore ç»“æœåˆ†æå·¥å…· v1.0                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    analyzer = ResultAnalyzer()
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶...\n")
    analyzer.load_all_results()
    
    if not analyzer.all_articles:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶ï¼")
        print("   è¯·å…ˆè¿è¡Œ ieee_crawler.py è¿›è¡Œçˆ¬å–")
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    analyzer.print_statistics()
    
    # å¯¼å‡ºé€‰é¡¹
    print("\n" + "="*80)
    print("ğŸ“¤ å¯¼å‡ºé€‰é¡¹")
    print("="*80)
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
        print("  1. å¯¼å‡ºä¸ºCSVï¼ˆæ¨èï¼Œå»é‡ï¼‰")
        print("  2. å¯¼å‡ºä¸ºCSVï¼ˆä¿ç•™é‡å¤ï¼‰")
        print("  3. å¯¼å‡ºä¸ºExcelï¼ˆéœ€è¦å®‰è£…pandasï¼‰")
        print("  4. å¯¼å‡ºæ£€ç´¢å¼ç»Ÿè®¡ä¿¡æ¯")
        print("  5. æŒ‰å…³é”®è¯æœç´¢")
        print("  6. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-6): ").strip()
        
        if choice == '1':
            analyzer.export_to_csv('all_articles_unique.csv', remove_duplicates=True)
        elif choice == '2':
            analyzer.export_to_csv('all_articles_all.csv', remove_duplicates=False)
        elif choice == '3':
            analyzer.export_to_excel('all_articles.xlsx', remove_duplicates=True)
        elif choice == '4':
            analyzer.export_query_stats()
        elif choice == '5':
            keyword = input("è¯·è¾“å…¥å…³é”®è¯: ").strip()
            if keyword:
                analyzer.search_by_keyword(keyword)
        elif choice == '6':
            print("\nğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")


if __name__ == "__main__":
    main()


