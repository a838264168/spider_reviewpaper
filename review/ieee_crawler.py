"""
IEEE Xplore æ–‡çŒ®æ£€ç´¢çˆ¬è™«
å®‰å…¨çˆ¬å–ï¼Œé¿å…å°IP
"""

import csv
import time
import random
import json
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ieee_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class IEEECrawler:
    def __init__(self, csv_file='IEEE_Xplore_æ£€ç´¢å¼æ±‡æ€»_ä¿®æ­£ç‰ˆ.csv'):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.csv_file = csv_file
        self.base_url = "https://ieeexplore.ieee.org/search/searchresult.jsp"
        
        # é¢‘ç‡æ§åˆ¶ï¼š60-120ç§’éšæœºé—´éš”ï¼ˆå®‰å…¨2å€ï¼‰
        self.min_delay = 60  
        self.max_delay = 120
        
        # é¡µé¢å†…å°å»¶è¿Ÿ
        self.small_delay_min = 3
        self.small_delay_max = 8
        
        # å¤šé¡µçˆ¬å–è®¾ç½®
        self.max_pages = 5  # æ¯ä¸ªæ£€ç´¢å¼æœ€å¤šçˆ¬å–5é¡µ
        self.results_per_page = 25  # IEEEé»˜è®¤æ¯é¡µ25æ¡
        
        # PDFä¸‹è½½è®¾ç½®
        self.download_pdf = True  # æ˜¯å¦ä¸‹è½½PDF
        self.pdf_dir = 'ieee_pdfs'  # PDFä¿å­˜ç›®å½•
        os.makedirs(self.pdf_dir, exist_ok=True)
        
        # ç»“æœä¿å­˜ç›®å½•
        self.output_dir = 'ieee_results'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¿›åº¦æ–‡ä»¶
        self.progress_file = 'crawl_progress.json'
        self.load_progress()
        
        # åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆå»¶è¿Ÿåˆ°å®é™…ä½¿ç”¨æ—¶ï¼‰
        self.driver = None
        
    def load_progress(self):
        """åŠ è½½çˆ¬å–è¿›åº¦"""
        if os.path.exists(self.progress_file):
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                self.progress = json.load(f)
            logging.info(f"åŠ è½½è¿›åº¦ï¼šå·²å®Œæˆ {len(self.progress.get('completed', []))} ä¸ªæ£€ç´¢å¼")
        else:
            self.progress = {'completed': [], 'failed': [], 'last_query_time': None}
    
    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.progress, f, ensure_ascii=False, indent=2)
    
    def init_driver(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        if self.driver is not None:
            return
        
        options = webdriver.ChromeOptions()
        
        # åçˆ¬è™«è®¾ç½®
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # è®¾ç½®User-Agent
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # å¯é€‰ï¼šæ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
        # options.add_argument('--headless')
        
        # è®¾ç½®ä¸‹è½½ç›®å½•å’Œè¡Œä¸º
        prefs = {
            'profile.default_content_setting_values': {
                'images': 2  # ç¦ç”¨å›¾ç‰‡
            },
            'download.default_directory': os.path.abspath(self.pdf_dir),  # ä¸‹è½½ç›®å½•
            'download.prompt_for_download': False,  # ä¸è¯¢é—®ä¸‹è½½ä½ç½®
            'download.directory_upgrade': True,
            'plugins.always_open_pdf_externally': True  # ä¸åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€PDF
        }
        options.add_experimental_option('prefs', prefs)
        
        try:
            # ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç† ChromeDriver
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': 'Object.defineProperty(navigator, "webdriver", {get: () => undefined})'
            })
            logging.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logging.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            logging.info("æç¤ºï¼šè¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨")
            raise
    
    def load_queries(self):
        """ä»CSVåŠ è½½æ£€ç´¢å¼"""
        queries = []
        with open(self.csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                query_id = row['ç¼–å·']
                query_text = row['æ£€ç´¢å¼'].strip('"')  # ç§»é™¤CSVçš„å¼•å·
                queries.append({
                    'id': query_id,
                    'text': query_text
                })
        logging.info(f"åŠ è½½äº† {len(queries)} ä¸ªæ£€ç´¢å¼")
        return queries
    
    def safe_delay(self, delay_type='large'):
        """å®‰å…¨å»¶è¿Ÿ"""
        if delay_type == 'large':
            # æŸ¥è¯¢é—´éš”ï¼š60-120ç§’
            delay = random.uniform(self.min_delay, self.max_delay)
            logging.info(f"ç­‰å¾… {delay:.1f} ç§’...")
        else:
            # é¡µé¢æ“ä½œé—´éš”ï¼š3-8ç§’
            delay = random.uniform(self.small_delay_min, self.small_delay_max)
        
        time.sleep(delay)
    
    def search_query(self, query_text):
        """æ‰§è¡Œå•ä¸ªæ£€ç´¢ï¼ˆæ”¯æŒå¤šé¡µï¼‰"""
        try:
            # æ„å»ºæœç´¢URL
            search_url = f"{self.base_url}?queryText={query_text}&newsearch=true"
            
            logging.info(f"æ­£åœ¨è®¿é—®ï¼š{search_url[:100]}...")
            self.driver.get(search_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            self.safe_delay('small')
            
            # ç­‰å¾…ç»“æœåŠ è½½
            wait = WebDriverWait(self.driver, 20)
            
            # å°è¯•è·å–ç»“æœæ•°é‡
            try:
                result_stats = wait.until(
                    EC.presence_of_element_located((By.CLASS_NAME, "Dashboard-statistics"))
                )
                total_results = result_stats.text
                logging.info(f"æ‰¾åˆ°ç»“æœï¼š{total_results}")
            except TimeoutException:
                logging.warning("æœªèƒ½è·å–ç»“æœç»Ÿè®¡ä¿¡æ¯")
                total_results = "æœªçŸ¥"
            
            # æå–å¤šé¡µæ–‡çŒ®åˆ—è¡¨
            all_articles = []
            
            for page_num in range(1, self.max_pages + 1):
                logging.info(f"æ­£åœ¨æå–ç¬¬ {page_num} é¡µ...")
                
                # æå–å½“å‰é¡µçš„æ–‡çŒ®
                page_articles = self.extract_articles()
                
                if not page_articles:
                    logging.warning(f"ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡çŒ®ï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                all_articles.extend(page_articles)
                logging.info(f"ç¬¬ {page_num} é¡µæå–äº† {len(page_articles)} ç¯‡æ–‡çŒ®ï¼ˆç´¯è®¡ï¼š{len(all_articles)} ç¯‡ï¼‰")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œå°è¯•ç¿»é¡µ
                if page_num < self.max_pages:
                    if not self.go_to_next_page():
                        logging.info("æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼Œåœæ­¢ç¿»é¡µ")
                        break
                    
                    # ç¿»é¡µåç­‰å¾…
                    self.safe_delay('small')
            
            logging.info(f"âœ“ å…±æå–äº† {len(all_articles)} ç¯‡æ–‡çŒ®ï¼ˆ{len(set(a['title'] for a in all_articles))} ç¯‡å»é‡ï¼‰")
            
            # ä¸‹è½½PDFï¼ˆå¦‚æœå¯ç”¨ï¼‰
            downloaded_count = 0
            if self.download_pdf and all_articles:
                logging.info(f"\nå¼€å§‹ä¸‹è½½ {len(all_articles)} ç¯‡æ–‡çŒ®çš„PDF...")
                
                for idx, article in enumerate(all_articles, 1):
                    success = self.download_article_pdf(article, idx, len(all_articles))
                    if success:
                        downloaded_count += 1
                    
                    # æ¯ç¯‡æ–‡ç« ä¸‹è½½åç­‰å¾…
                    if idx < len(all_articles):
                        self.safe_delay('small')
                
                logging.info(f"âœ“ PDFä¸‹è½½å®Œæˆï¼šæˆåŠŸ {downloaded_count}/{len(all_articles)} ç¯‡")
            
            return {
                'success': True,
                'total_results': total_results,
                'articles_count': len(all_articles),
                'articles': all_articles,
                'pdfs_downloaded': downloaded_count
            }
            
        except TimeoutException:
            logging.error("é¡µé¢åŠ è½½è¶…æ—¶")
            return {'success': False, 'error': 'timeout'}
        except Exception as e:
            logging.error(f"æœç´¢å‡ºé”™ï¼š{e}")
            return {'success': False, 'error': str(e)}
    
    def extract_articles(self):
        """æå–å½“å‰é¡µé¢çš„æ–‡çŒ®ä¿¡æ¯"""
        articles = []
        
        try:
            # ç­‰å¾…æ–‡çŒ®åˆ—è¡¨åŠ è½½
            wait = WebDriverWait(self.driver, 10)
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "List-results-items")))
            
            # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‰€æœ‰ç»“æœï¼ˆIEEEä½¿ç”¨æ‡’åŠ è½½ï¼‰
            logging.info("æ­£åœ¨æ»šåŠ¨é¡µé¢åŠ è½½æ‰€æœ‰ç»“æœ...")
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            for _ in range(3):  # æœ€å¤šæ»šåŠ¨3æ¬¡
                # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # ç­‰å¾…åŠ è½½
                
                # è®¡ç®—æ–°çš„æ»šåŠ¨é«˜åº¦
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break  # æ²¡æœ‰æ–°å†…å®¹äº†
                last_height = new_height
            
            # æ»šå›é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # è·å–æ‰€æœ‰æ–‡çŒ®é¡¹
            article_elements = self.driver.find_elements(By.CLASS_NAME, "result-item")
            logging.info(f"åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡çŒ®é¡¹")
            
            for idx, element in enumerate(article_elements, 1):
                try:
                    # æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆä½¿ç”¨h3 aï¼‰
                    try:
                        title_elem = element.find_element(By.CSS_SELECTOR, "h3 a")
                        title = title_elem.text.strip()
                        link = title_elem.get_attribute('href')
                    except NoSuchElementException:
                        # å¤‡ç”¨æ–¹æ¡ˆ
                        title_elem = element.find_element(By.CLASS_NAME, "result-item-title")
                        title = title_elem.text.strip()
                        link = title_elem.find_element(By.TAG_NAME, "a").get_attribute('href')
                    
                    # æå–ä½œè€…
                    try:
                        authors = element.find_element(By.CLASS_NAME, "author").text.strip()
                    except NoSuchElementException:
                        authors = "N/A"
                    
                    # æå–å‘è¡¨ä¿¡æ¯
                    try:
                        publisher_info = element.find_element(By.CLASS_NAME, "publisher-info-container").text.strip()
                    except NoSuchElementException:
                        publisher_info = "N/A"
                    
                    # æå–å¹´ä»½
                    try:
                        year = element.find_element(By.CLASS_NAME, "detail-info-year").text.strip()
                    except NoSuchElementException:
                        year = "N/A"
                    
                    # æå–æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
                    try:
                        abstract = element.find_element(By.CLASS_NAME, "description").text.strip()
                    except NoSuchElementException:
                        abstract = "N/A"
                    
                    # æå–æ–‡æ¡£IDï¼ˆç”¨äºå‘½åPDFï¼‰
                    doc_id = link.split('/')[-2] if '/' in link else f"doc_{idx}"
                    
                    article = {
                        'title': title,
                        'link': link,
                        'authors': authors,
                        'publisher_info': publisher_info,
                        'year': year,
                        'abstract': abstract,
                        'doc_id': doc_id,
                        'pdf_downloaded': False,
                        'pdf_path': None
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    logging.warning(f"æå–ç¬¬ {idx} ç¯‡æ–‡çŒ®æ—¶å‡ºé”™ï¼š{e}")
                    continue
            
            logging.info(f"æˆåŠŸæå– {len(articles)} ç¯‡æ–‡çŒ®ä¿¡æ¯")
            
        except Exception as e:
            logging.error(f"æå–æ–‡çŒ®åˆ—è¡¨å¤±è´¥ï¼š{e}")
        
        return articles
    
    def go_to_next_page(self):
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾å¹¶ç‚¹å‡»"ä¸‹ä¸€é¡µ"æŒ‰é’®
            next_buttons = self.driver.find_elements(By.XPATH, "//button[@aria-label='Next page']")
            
            if not next_buttons:
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾åˆ†é¡µå™¨ä¸­çš„ä¸‹ä¸€é¡µé“¾æ¥
                next_buttons = self.driver.find_elements(By.XPATH, "//a[contains(@class, 'next-page')]")
            
            if not next_buttons:
                # æ–¹æ³•3ï¼šæŸ¥æ‰¾åŒ…å«">"æˆ–"Next"æ–‡æœ¬çš„æŒ‰é’®
                next_buttons = self.driver.find_elements(By.XPATH, "//button[contains(text(), 'Next')]")
            
            if not next_buttons:
                # æ–¹æ³•4ï¼šé€šè¿‡CSSé€‰æ‹©å™¨æŸ¥æ‰¾
                next_buttons = self.driver.find_elements(By.CSS_SELECTOR, ".pagination .next, .pagination li:last-child a")
            
            for button in next_buttons:
                try:
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨ï¼ˆæ²¡æœ‰disabledå±æ€§ï¼‰
                    if button.is_enabled() and button.is_displayed():
                        # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                        self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                        time.sleep(1)
                        
                        # ç‚¹å‡»
                        button.click()
                        logging.info("âœ“ æˆåŠŸç¿»é¡µ")
                        return True
                except Exception as e:
                    logging.debug(f"å°è¯•ç‚¹å‡»æŒ‰é’®å¤±è´¥ï¼š{e}")
                    continue
            
            logging.warning("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®")
            return False
            
        except Exception as e:
            logging.error(f"ç¿»é¡µå¤±è´¥ï¼š{e}")
            return False
    
    def download_article_pdf(self, article, current_idx, total_count):
        """ä¸‹è½½å•ç¯‡æ–‡ç« çš„PDFï¼ˆä¸¤æ­¥æµç¨‹ï¼šæ‰“å¼€æŸ¥çœ‹å™¨ -> ä¸‹è½½ï¼‰"""
        doc_id = article.get('doc_id', 'unknown')
        title = article.get('title', 'Untitled')[:50]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
        link = article.get('link', '')
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_filename = "".join(c for c in f"{doc_id}_{title}" if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_filename = safe_filename[:100]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        pdf_path = os.path.join(self.pdf_dir, f"{safe_filename}.pdf")
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 1000:  # è‡³å°‘1KB
            logging.info(f"[{current_idx}/{total_count}] PDFå·²å­˜åœ¨ï¼š{safe_filename}.pdf")
            article['pdf_downloaded'] = True
            article['pdf_path'] = pdf_path
            return True
        
        try:
            logging.info(f"[{current_idx}/{total_count}] æ­£åœ¨ä¸‹è½½ï¼š{title[:40]}...")
            
            # ç¬¬ä¸€æ­¥ï¼šè®¿é—®æ–‡ç« é¡µé¢ï¼Œæ‰¾åˆ°PDFæŸ¥çœ‹å™¨é“¾æ¥
            self.driver.get(link)
            time.sleep(3)
            
            # æŸ¥æ‰¾PDFæŸ¥çœ‹å™¨é“¾æ¥ï¼ˆstamp.jspï¼‰
            pdf_viewer_link = None
            try:
                # æ–¹æ³•1ï¼šæŸ¥æ‰¾åŒ…å«stamp.jspçš„é“¾æ¥
                pdf_links = self.driver.find_elements(By.XPATH, "//a[contains(@href, 'stamp.jsp')]")
                if pdf_links:
                    pdf_viewer_link = pdf_links[0].get_attribute('href')
                    logging.info(f"  âœ“ æ‰¾åˆ°PDFæŸ¥çœ‹å™¨é“¾æ¥")
            except:
                pass
            
            if not pdf_viewer_link:
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾PDFæŒ‰é’®
                try:
                    pdf_button = self.driver.find_element(By.CSS_SELECTOR, "[class*='pdf']")
                    pdf_viewer_link = pdf_button.get_attribute('href')
                except:
                    pass
            
            if not pdf_viewer_link:
                logging.warning(f"  âœ— æœªæ‰¾åˆ°PDFæŸ¥çœ‹å™¨é“¾æ¥ï¼š{title[:40]}")
                return False
            
            # ç¬¬äºŒæ­¥ï¼šæ‰“å¼€PDFæŸ¥çœ‹å™¨é¡µé¢å¹¶æå–iframeä¸­çš„PDF URL
            logging.info(f"  â†’ æ‰“å¼€PDFæŸ¥çœ‹å™¨...")
            self.driver.get(pdf_viewer_link)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # ç¬¬ä¸‰æ­¥ï¼šæŸ¥æ‰¾iframeä¸­çš„getPDF.jspé“¾æ¥
            pdf_download_url = None
            try:
                # æ–¹æ³•1ï¼šæŸ¥æ‰¾iframeçš„srcå±æ€§
                iframes = self.driver.find_elements(By.TAG_NAME, "iframe")
                for iframe in iframes:
                    src = iframe.get_attribute('src')
                    if src and 'getPDF.jsp' in src:
                        pdf_download_url = src
                        logging.info(f"  âœ“ æ‰¾åˆ°PDFä¸‹è½½URLï¼ˆiframeï¼‰")
                        break
                
                # æ–¹æ³•2ï¼šä»é¡µé¢æºç ä¸­æå–
                if not pdf_download_url:
                    page_source = self.driver.page_source
                    import re
                    match = re.search(r'https://[^"\']*?getPDF\.jsp[^"\']*', page_source)
                    if match:
                        pdf_download_url = match.group(0).replace('&amp;', '&')
                        logging.info(f"  âœ“ æ‰¾åˆ°PDFä¸‹è½½URLï¼ˆæºç ï¼‰")
                        
            except Exception as e:
                logging.debug(f"  æŸ¥æ‰¾PDF URLå¤±è´¥ï¼š{e}")
            
            if not pdf_download_url:
                logging.warning(f"  âœ— æœªæ‰¾åˆ°PDFä¸‹è½½URLï¼š{title[:40]}")
                return False
            
            # ç¬¬å››æ­¥ï¼šä½¿ç”¨requestsç›´æ¥ä¸‹è½½PDF
            logging.info(f"  â†’ å¼€å§‹ä¸‹è½½PDF...")
            try:
                import requests
                # å¤åˆ¶æµè§ˆå™¨çš„cookiesä»¥ä¿æŒä¼šè¯
                cookies = {}
                for cookie in self.driver.get_cookies():
                    cookies[cookie['name']] = cookie['value']
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': pdf_viewer_link
                }
                
                response = requests.get(pdf_download_url, headers=headers, cookies=cookies, timeout=30)
                
                if response.status_code == 200 and len(response.content) > 1000:
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDFæ–‡ä»¶
                    if response.content[:4] == b'%PDF':
                        with open(pdf_path, 'wb') as f:
                            f.write(response.content)
                        article['pdf_downloaded'] = True
                        article['pdf_path'] = pdf_path
                        file_size = len(response.content) / 1024
                        logging.info(f"  âœ“ ä¸‹è½½æˆåŠŸï¼š{safe_filename}.pdf ({file_size:.1f} KB)")
                        return True
                    else:
                        logging.warning(f"  âœ— å“åº”ä¸æ˜¯PDFæ–‡ä»¶ï¼ˆå¯èƒ½éœ€è¦è®¢é˜…ï¼‰")
                        return False
                else:
                    logging.warning(f"  âœ— ä¸‹è½½å¤±è´¥ï¼šHTTP {response.status_code}")
                    return False
                    
            except Exception as e:
                logging.error(f"  âœ— ä¸‹è½½å‡ºé”™ï¼š{str(e)[:100]}")
                return False
            
        except Exception as e:
            logging.error(f"  âœ— ä¸‹è½½å¤±è´¥ï¼š{str(e)[:100]}")
            return False
    
    def wait_for_download(self, expected_path, filename, timeout=30):
        """ç­‰å¾…æ–‡ä»¶ä¸‹è½½å®Œæˆ"""
        import glob
        
        # ç­‰å¾…ä¸‹è½½å¼€å§‹å’Œå®Œæˆ
        for i in range(timeout):
            time.sleep(1)
            
            # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(expected_path) and os.path.getsize(expected_path) > 1000:
                logging.debug(f"  æ–‡ä»¶å·²ä¸‹è½½ï¼š{os.path.getsize(expected_path)} bytes")
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰.crdownloadä¸´æ—¶æ–‡ä»¶ï¼ˆChromeä¸‹è½½ä¸­ï¼‰
            temp_files = glob.glob(os.path.join(self.pdf_dir, "*.crdownload"))
            if not temp_files and i > 5:  # 5ç§’åè¿˜æ²¡æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œå¯èƒ½ä¸‹è½½å¤±è´¥
                # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰æ–°ä¸‹è½½çš„PDF
                recent_pdfs = glob.glob(os.path.join(self.pdf_dir, "*.pdf"))
                for pdf in recent_pdfs:
                    if os.path.getmtime(pdf) > time.time() - 30:  # 30ç§’å†…çš„æ–‡ä»¶
                        # å¯èƒ½æ˜¯åˆšä¸‹è½½çš„ï¼Œé‡å‘½åä¸ºæœŸæœ›çš„æ–‡ä»¶å
                        if not os.path.exists(expected_path):
                            try:
                                os.rename(pdf, expected_path)
                                logging.debug(f"  é‡å‘½åæ–‡ä»¶ï¼š{os.path.basename(pdf)} -> {os.path.basename(expected_path)}")
                                return True
                            except:
                                pass
        
        logging.warning(f"  ä¸‹è½½è¶…æ—¶ï¼š{filename}")
        return False
    
    def save_results(self, query_id, query_text, result_data):
        """ä¿å­˜å•ä¸ªæ£€ç´¢å¼çš„ç»“æœ"""
        filename = f"{self.output_dir}/query_{query_id}_results.json"
        
        output_data = {
            'query_id': query_id,
            'query_text': query_text,
            'crawl_time': datetime.now().isoformat(),
            'total_results': result_data.get('total_results', 'N/A'),
            'articles_count': result_data.get('articles_count', 0),
            'articles': result_data.get('articles', [])
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ç»“æœå·²ä¿å­˜åˆ°ï¼š{filename}")
    
    def run(self, start_from=1):
        """è¿è¡Œçˆ¬è™«"""
        try:
            # åˆå§‹åŒ–æµè§ˆå™¨
            self.init_driver()
            
            # åŠ è½½æ£€ç´¢å¼
            queries = self.load_queries()
            
            # è¿‡æ»¤å·²å®Œæˆçš„
            remaining_queries = [q for q in queries if q['id'] not in self.progress['completed']]
            
            if start_from > 1:
                remaining_queries = [q for q in remaining_queries if int(q['id']) >= start_from]
            
            logging.info(f"å¾…çˆ¬å–ï¼š{len(remaining_queries)} ä¸ªæ£€ç´¢å¼")
            
            for idx, query in enumerate(remaining_queries, 1):
                query_id = query['id']
                query_text = query['text']
                
                logging.info(f"\n{'='*60}")
                logging.info(f"è¿›åº¦ï¼š{idx}/{len(remaining_queries)} | æ£€ç´¢å¼ #{query_id}")
                logging.info(f"æ£€ç´¢å¼ï¼š{query_text[:100]}...")
                logging.info(f"{'='*60}\n")
                
                # æ‰§è¡Œæœç´¢
                result = self.search_query(query_text)
                
                if result['success']:
                    # ä¿å­˜ç»“æœ
                    self.save_results(query_id, query_text, result)
                    
                    # æ ‡è®°ä¸ºå®Œæˆ
                    self.progress['completed'].append(query_id)
                    self.progress['last_query_time'] = datetime.now().isoformat()
                    self.save_progress()
                    
                    logging.info(f"âœ“ æ£€ç´¢å¼ #{query_id} å®Œæˆ")
                else:
                    # æ ‡è®°ä¸ºå¤±è´¥
                    self.progress['failed'].append({
                        'query_id': query_id,
                        'error': result.get('error', 'unknown'),
                        'time': datetime.now().isoformat()
                    })
                    self.save_progress()
                    
                    logging.error(f"âœ— æ£€ç´¢å¼ #{query_id} å¤±è´¥")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªï¼Œåˆ™ç­‰å¾…
                if idx < len(remaining_queries):
                    self.safe_delay('large')
            
            logging.info("\n" + "="*60)
            logging.info("âœ… æ‰€æœ‰æ£€ç´¢å¼çˆ¬å–å®Œæˆï¼")
            logging.info(f"æˆåŠŸï¼š{len(self.progress['completed'])} ä¸ª")
            logging.info(f"å¤±è´¥ï¼š{len(self.progress['failed'])} ä¸ª")
            logging.info("="*60)
            
        except KeyboardInterrupt:
            logging.info("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            logging.error(f"çˆ¬è™«è¿è¡Œå‡ºé”™ï¼š{e}")
        finally:
            if self.driver:
                self.driver.quit()
                logging.info("æµè§ˆå™¨å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           IEEE Xplore æ–‡çŒ®æ£€ç´¢çˆ¬è™« v1.0                  â•‘
    â•‘                                                           â•‘
    â•‘  âš ï¸  å®‰å…¨è®¾ç½®ï¼š                                           â•‘
    â•‘     - æŸ¥è¯¢é—´éš”ï¼š60-120ç§’éšæœºå»¶è¿Ÿ                         â•‘
    â•‘     - æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º                                 â•‘
    â•‘     - è‡ªåŠ¨ä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬                         â•‘
    â•‘                                                           â•‘
    â•‘  ğŸ“ è¾“å‡ºç›®å½•ï¼šieee_results/                              â•‘
    â•‘  ğŸ“‹ æ—¥å¿—æ–‡ä»¶ï¼šieee_crawler.log                           â•‘
    â•‘  ğŸ’¾ è¿›åº¦æ–‡ä»¶ï¼šcrawl_progress.json                        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = IEEECrawler()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡
    if crawler.progress['completed']:
        print(f"\nğŸ“Š æ£€æµ‹åˆ°ä¹‹å‰çš„çˆ¬å–è¿›åº¦ï¼š")
        print(f"   å·²å®Œæˆï¼š{len(crawler.progress['completed'])} ä¸ªæ£€ç´¢å¼")
        print(f"   å¤±è´¥ï¼š{len(crawler.progress['failed'])} ä¸ªæ£€ç´¢å¼")
        
        choice = input("\næ˜¯å¦ç»§ç»­ä¹‹å‰çš„è¿›åº¦ï¼Ÿ(y/n): ").strip().lower()
        if choice != 'y':
            choice = input("æ˜¯å¦ä»å¤´å¼€å§‹ï¼Ÿè¿™å°†æ¸…é™¤ä¹‹å‰çš„è¿›åº¦ (y/n): ").strip().lower()
            if choice == 'y':
                crawler.progress = {'completed': [], 'failed': [], 'last_query_time': None}
                crawler.save_progress()
                print("âœ“ è¿›åº¦å·²é‡ç½®")
    
    print("\nğŸš€ å¼€å§‹çˆ¬å–...\n")
    
    # è¿è¡Œçˆ¬è™«
    crawler.run()


if __name__ == "__main__":
    main()

